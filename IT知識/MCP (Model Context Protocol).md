---
tags:
  - program
aliases:
  - mcp
---
## MCPとは
Model Context Protocol (MCP)は、Anthropic社によって2024年11月に発表された、AIアシスタントを外部のシステムやデータソースに接続するためのオープンな標準規格です。
AIアプリケーションにとっての「USB-Cポート」のようなものと考えると理解しやすく、個別のツールごとに専用のアダプター（連携コード）を作る必要をなくし、標準化された方法で安全に接続することを可能にします。

## MCPが解決する課題

従来、AIモデル（例：ClaudeやChatGPT）を外部ツール（例：Google Drive、Slack、社内データベース）に接続するには、ツールごとに固有のコードを書く必要がありました。AIアプリが増え、連携したいツールも増えると、その組み合わせの数だけ開発が必要になる「N対Mの統合問題」が発生していました。 MCPはこの問題を解決し、開発者が一度MCPサーバーを構築すれば、あらゆるMCP対応AIアプリケーションからそのツールを利用できるようにします。

## [アーキテクチャ](https://chatgpt-enterprise.jp/blog/mcp/)

![[image-1024x529.webp]]
## 1. MCPホスト

- **役割**: ユーザーとAIの橋渡し役 
- **具体例**: Claude Desktopアプリ、ChatGPTプラグイン基盤など

ホストはAIアプリケーションの中核部分で、ユーザーからの指示を受け取り、必要に応じて適切なツールを呼び出す司令塔の役割を果たします。ユーザーインターフェースを提供し、AIモデルとの対話を管理します。

## 2. MCPサーバー

- **役割**: 機能やデータの提供者 
- **具体例**: 天気予報API、翻訳サービス、社内データベースなど

サーバーは特定の機能やデータを提供する側で、標準化されたMCPプロトコルに従ってリクエストを受け付け、結果を返します。これにより、様々な企業やデベロッパーが独自のツールを共通形式で公開できます。

## 3. MCPクライアント

- **役割**: ツールの呼び出し役 
- **具体例**: Cline（VS Code拡張）、Claude Desktopの内部機能など

クライアントは、AIエージェントの指示に基づいてMCPサーバーにリクエストを送信し、結果を受け取る役割を担います。ユーザーの意図を適切なツール呼び出しに変換する重要な役割を果たします。


##  [MCP の仕組み](https://cloud.google.com/discover/what-is-model-context-protocol?hl=ja)

Model Context Protocol の本質は、LLMがクエリへの回答やタスクの完了といった目的のために、外部ツールにサポートをリクエストできるようにすることです。AI アシスタントに「データベースから最新の売上レポートを見つけて、上司にメールで送って」と指示したとします。

MCP でこの問題を処理する仕組みを簡単に説明します。

1. **リクエストとツールの検出:** LLMは、単独でデータベースにアクセスしたり、メールを送信したりできないことを理解しています。MCPクライアントを使用して利用可能なツールを検索し、MCPサーバーに登録されている 2 つの関連ツール（database_queryツールとemail_senderツール）を検出します。
2. **ツールの呼び出し:** LLMは、こうしたツールを使用するための構造化されたリクエストを生成します。まず、レポート名を指定してdatabase_queryツールを呼び出します。MCPクライアントは、このリクエストを適切な MCPサーバーに送信します。
3. **外部アクションとデータの返送:** MCPサーバーはリクエストを受け取り、それを会社のデータベース用の安全な SQLクエリに変換して、売上レポートを取得します。その後、このデータをフォーマットして LLM に送り返します。
4. **2 回目のアクションと回答の生成:** レポートデータを受け取ったLLMは、マネージャーのメールアドレスとレポートの内容を指定してemail_senderツールを呼び出します。メールが送信されると、MCPサーバーはアクションが完了したことを確認します。
5. **最終確認:** LLMが次のような最終的な回答を返します。「最新の売上レポートを見つけたので、上司にメールで送信しました。」

## MCP と RAG の比較

Model Context Protocol（MCP）と[検索拡張生成](https://cloud.google.com/use-cases/retrieval-augmented-generation)（RAG）はどちらも外部情報を使用してLLMを改善しますが、その方法と目的は異なります。RAGはテキストを作成するための情報を検索して利用するのに対し、MCPはインタラクションとアクションのためのより広範なシステムです。

| **機能**   | **Model Context Protocol（MCP）**                                                                | **検索拡張生成（RAG）**                                                                         |
| -------- | ---------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| 最終目標     | LLMが外部ツール、データソース、サービスにアクセスしてやり取りし、情報検索と並行してアクションを実行するための双方向通信を標準化します。                          | 回答を生成する前に信頼できるナレッジベースから関連情報を取得して、LLMの回答を強化します。                                          |
| メカニズム    | LLMアプリケーションが外部関数を呼び出すための、または専用サーバーから構造化データをリクエストするための標準化されたプロトコルを定義し、アクションと動的なコンテキスト統合を可能にします。 | ユーザーのクエリを使用して、ナレッジベースまたはデータソースから情報を取得する情報検索コンポーネントを組み込んでいます。取得した情報により、LLMのプロンプトが補強されます。 |
| 出力タイプ    | LLMがツールの構造化された呼び出しを生成し、結果を受け取って、その結果とアクションに基づいて人間が読めるテキストを生成できるようにします。リアルタイムデータや関数も扱うことができます。  | LLMは、外部ドキュメントからクエリに関連するテキストで拡張されたトレーニングデータに基づいて回答を生成します。多くの場合、事実の正確性を重視します。             |
| インタラクション | 外部システムとのアクティブなインタラクションとタスクの実行を目的として設計されており、LLMが外部の機能を「使用」するための「文法」を提供します。                      | 主に、テキスト生成に役立つ情報を受動的に取得するために使用されます。通常は、外部システム内でアクションを実行するために使用されることはありません。               |
| 標準化      | AIアプリケーションがLLMにコンテキストを提供する方法に関するオープン スタンダード。統合を標準化し、カスタムAPIの必要性を低減します。                         | LLMを改善するための手法またはフレームワークですが、異なるベンダーやシステム間でのツールのやり取りのための汎用プロトコルではありません。                   |
| ユースケース   | タスクの実行（フライトの予約、CRMの更新、コードの実行など）、リアルタイム データの取得、高度なインテグレーションを行う AIエージェント。                        | 質問応答システム、最新の事実にもとづく情報を提供するchatbot、ドキュメントの要約、テキスト生成におけるハルシネーションの低減。                      |

## MCP を使用するメリット

モデルコンテキストプロトコルは、AIを活用したアプリケーションの開発とデプロイにいくつかの潜在的なメリットをもたらし、LLMの汎用性、信頼性、能力を高めます。

### ハルシネーションの低減

LLMは、リアルタイムの情報ではなくトレーニング データに基づいて回答を予測するため、その性質上、事実をでっち上げたり、もっともらしいが最終的には不正確な情報を生成したりすることがあります（[ハルシネーション](https://cloud.google.com/discover/what-are-ai-hallucinations)）。MCPは、LLMが外部の信頼できるデータソースにアクセスするための明確な方法を提供することで、この問題を軽減し、回答の信頼性を高めます。

### AI の実用性と自動化の向上

このプロトコルにより、AIはより多くのことを行い、自律的に動作できるようになります。通常、LLMはトレーニングされた内容しか把握しておらず、その内容はすぐに古くなる可能性があります。しかし、MCPを使用すると、LLMはビジネス ソフトウェア、コンテンツ リポジトリ、開発環境といった多数の既製のツールや調査に接続できます。つまり、AIは、CRMシステムの顧客情報の更新、オンラインでの最新の出来事の検索、特別な計算の実行など、現実世界とのやり取りを伴うより複雑な作業を処理できるようになります。これらの外部ツールに直接接続することで、LLM は単なるチャットプログラムではなく、自律的に行動できるスマートエージェントになります。つまり、自動化できることが大幅に増えます。

### AI の接続が容易に

MCPが登場する前は、LLMをさまざまな外部データソースやツールに接続するのは容易でななく、通常は特別な接続を必要としたり、各ベンダー固有の方法を使用したりしていました。その結果、システムは複雑で煩雑なものとなり、新しいモデルやツールが追加されるたびに必要なカスタム接続の数が急速に増えるため、しばしば「N x M」問題と呼ばれていました。MCPは、USB-Cポートでデバイスの接続がシンプルになるのと同じように、これらの接続を容易にする共通のオープンスタンダードを提供します。このシンプルな方法により、開発コストを削減し、AIアプリケーションの作成を迅速化し、接続性に優れた AI環境を構築できます。また、開発者は、LLMプロバイダをより簡単に切り替えたり、大きな変更を加えることなく新しいツールを追加したりできます。

## MCP とセキュリティ

Model Context Protocol は、LLMを外部システムに接続することでLLMの機能を向上させますが、重要なセキュリティ上の考慮事項が生じる可能性もあります。MCPはあらゆるデータにアクセスでき、接続されたツールを通じてコードを実行できる可能性があるため、強固なセキュリティが不可欠です。

MCPの主なセキュリティ原則は次のとおりです。

- **ユーザーの同意と管理:** ユーザーは、MCPを通じてLLMが実行するすべてのアクションとデータアクセスを明確に理解し、同意する必要があります。ユーザーは、理想的には、使いやすい認証画面を通じて、共有するデータと実行するアクションを制御できる必要があります。
- **データプライバシー:** ユーザーデータをMCPサーバーに公開する前に、ホストはユーザーから明確な許可を得る必要があります。LLMは大量のデータを処理するため、特に機密データは適切なアクセス制御で保護し、偶発的な漏洩や共有を防ぐ必要があります。暗号化と強力なアクセス制御ルールを使用することが不可欠です。
- **ツールの安全性:** MCPを介してリンクされたツールは、コードの実行に使用できます。デベロッパーは、ツール記述が信頼できるサーバーから提供されたものでない限りは信用しないでください。ユーザーは、ツールが使用される前に権限を付与し、そのツールの機能について理解してから実行を許可する必要があります。
- **安全な出力処理:** MCPとのやり取りから得られたLLMの出力は、その出力がユーザーに表示される場合、クロスサイト スクリプティング（XSS）やその他のウェブ アプリケーション攻撃などのセキュリティ上の問題を防ぐために、慎重に扱う必要があります。入力を適切にクリーンアップして出力をフィルタリングし、プロンプトに機密データが含まれることがないようにすることが重要です。
- **サプライ チェーンのセキュリティ:** MCPサーバーと、MCPサーバーが接続する外部ツールの信頼性は非常に重要です。組織は、LLM サプライ チェーンのすべての部分が安全であることを確認し、バイアスのかかった結果、セキュリティ侵害、障害を防止する必要があります。
- **モニタリングと監査:** LLMのアクティビティと MCPサーバーとのやり取りを定期的にチェックすることで、異常な動作や潜在的な不正使用を発見できます。強力なロギングおよび監査システムをセットアップすることで、データの移動やツールの使用状況を追跡できます。これは、セキュリティ インシデントに対応する際に役立ちます。

これらの原則を守ることで、デベロッパーは潜在的なリスクから保護しながら、MCPの力を活用できます。